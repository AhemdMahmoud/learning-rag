# -*- coding: utf-8 -*-
"""Rag_using_opensource.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v4o4RJLdBCDNkSmYM4IGEHUd_qTS4WLn
"""

!pip -q install langchain langchain_community pypdf fastembed chromadb

# get the huggingface token
import os
from getpass import getpass
from google.colab import userdata

HF_TOKEN = getpass("Huggingface Token : ")
os.environ['HUGGINGFACEHUB_API_TOKEN'] = HF_TOKEN

from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("/content/Ahmed-MahmoudL-CVNIl.pdf")
data = loader.load()

data[0].page_content

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=0
)

docs = text_splitter.split_documents(data)

len(docs)

docs[0]

docs[1]

"""# Embding_model"""

from langchain.embeddings.fastembed import FastEmbedEmbeddings

model_name = "thenlper/gte-large"
embedding_model = FastEmbedEmbeddings(model_name="thenlper/gte-large")

"""# Vector_db"""

from langchain.vectorstores import Chroma

vectordb = Chroma.from_documents(
    docs,
    embedding_model,
    persist_directory="./chroma_db",

)

Qa= "WWhat is name of unversity"

vectordb.similarity_search(Qa)[0].page_content

# initialize the retriever
retriever = vectordb.as_retriever(search_type="mmr", search_kwargs={"k": 2, "fetch_k": 3})

retriever.invoke("WWhat is name of unversity")[0].page_content

"""## it's very bad and i except tis becuse bad embding model"""

!pip install -q  langchain-huggingface

from langchain_huggingface import HuggingFaceEndpoint

# import the question-answering chain and Huggingface Hub LLM
from langchain_huggingface import HuggingFaceEndpoint

# define the llm
llm = HuggingFaceEndpoint(
    repo_id="mistralai/Mistral-7B-Instruct-v0.2",
    task="text-generation",
    temperature=0.1,
    max_new_tokens=512,
    return_full_text=False,
    repetition_penalty=1.1,
    top_p=0.9
)

from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

template = """
<s>[INST]
You are an AI Assistant that follows instructions extremely well.
Please be truthful and give direct answers. Please tell 'I don't know' if user query is not in CONTEXT
[/INST]
CONTEXT: {context}
</s>
[INST]
{query}
[/INST]
"""

prompt = ChatPromptTemplate.from_template(template)

output_parser = StrOutputParser()

chain = (
{"context": retriever, "query": RunnablePassthrough()}
| prompt
| llm
| output_parser
)